{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eafe2c4",
   "metadata": {},
   "source": [
    "# Data PreparationÂ¶\n",
    "### In this notebook we will prepare our data for our search function to use.\n",
    "### Currently we have data stored in four different csv files.\n",
    "<ul>\n",
    "<li>AnimeList.csv</li>\n",
    "<li>Anime_data.csv</li>\n",
    "</ul>\n",
    " ###It can be computationally expensive to produce analysis results from multiple data-sources for incomming stream of requests.\n",
    "So we will prepare our data and save it in an easily searchable structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2ee71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed modules...\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2be09",
   "metadata": {},
   "source": [
    "## Define Paths to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc6195e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_AnimeList   = f\"{getcwd()}/Data_Sets/AnimeList.csv\"\n",
    "PATH_Anime_data  = f\"{getcwd()}/Data_Sets/Anime_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293ed68",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "<ul>\n",
    "    <li>## Get data in dataframes.</li>\n",
    "    <li>## Convert data to a single dictionary.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ef7c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS : ['anime_id', 'title', 'title_english', 'title_japanese', 'title_synonyms', 'image_url', 'type', 'source', 'episodes', 'status', 'airing', 'aired_string', 'aired', 'duration', 'rating', 'score', 'scored_by', 'rank', 'popularity', 'members', 'favorites', 'background', 'premiered', 'broadcast', 'related', 'producer', 'licensor', 'studio', 'genre', 'opening_theme', 'ending_theme']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Read data from AnimeList.csv\n",
    "\"\"\"\n",
    "df_animelist            = pd.read_csv(PATH_AnimeList)\n",
    "animelist_table_columns = df_animelist.columns.tolist()\n",
    "print(f\"COLUMNS : {animelist_table_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3756dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS : ['anime_id', 'name', 'genre', 'type', 'episodes', 'rating', 'members']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Read data from Anime_data.csv\n",
    "\"\"\"\n",
    "df_anime_data        = pd.read_csv(PATH_Anime_data)\n",
    "anime_data_table_columns = df_anime_data.columns.tolist()\n",
    "print(f\"COLUMNS : {anime_data_table_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7744152",
   "metadata": {},
   "source": [
    "<ul><li> anime_id is a common column in both the tables so we will use it as a primary search-keyas well as a sort key.</li>\n",
    "<li>A user will always search an anime by its title so we will create a Global secondary index to be able to perform search our datastore.\n",
    "it will obviously take some extra space but almost negligible as compared to the size of the original data.\n",
    "In addition, It will make our searching faster and efficient so it's a good deal.</li><ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e018e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is True  that the column 'anime_id' has unique values for all entries in Animelist dataframe.\n",
      "It is True  that the column 'anime_id' has unique values for all entries in Anime_data dataframe.\n"
     ]
    }
   ],
   "source": [
    "print(f\"It is {pd.Series(df_animelist['anime_id']).is_unique}  that the column 'anime_id' has unique values for all entries in Animelist dataframe.\")\n",
    "print(f\"It is {pd.Series(df_anime_data['anime_id']).is_unique}  that the column 'anime_id' has unique values for all entries in Anime_data dataframe.\")\n",
    "\n",
    "# Sort AnimeList dataframe on the basis of anime_id as anime_id is unique for all entries...\n",
    "df_animelist_sorted = df_animelist.sort_values(by=['anime_id'])\n",
    "\n",
    "# Sort Anime_data dataframe on the basis of anime_id as anime_id is unique for all entries...\n",
    "df_anime_data_sorted  = df_anime_data.sort_values(by=['anime_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "356bf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from movies dataframe...\n",
    "animelist_Ids    = df_animelist_sorted[\"anime_id\"].tolist()\n",
    "animelist_Titles = df_animelist_sorted[\"title\"].tolist()\n",
    "animelist_Genres = df_animelist_sorted[\"genre\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9be844b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "animelistDict         = {}\n",
    "global_secondaryIndex = {}\n",
    "for idx, animelist_Id  in enumerate(animelist_Ids):\n",
    "    animelistDict[animelist_Id] = {\n",
    "        \"genre\" : animelist_Genres[idx],\n",
    "    }\n",
    "    \n",
    "    global_secondaryIndex[animelist_Titles[idx]] = animelist_Id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba702018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete veriables which are no longer in use while holding large amount of data.\n",
    "del animelist_Ids \n",
    "del animelist_Titles\n",
    "del animelist_Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b21c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, adding the data in the animelistDict...\n",
    "#adding anime reviews\n",
    "for idx,_ in animelistDict.items():\n",
    "    try   : animelistDict[idx][\"member_rating\"] = Anime_data['rating'][Anime_data[anime_id]==idx]\n",
    "    except: \n",
    "        try   : animelistDict[idx][\"member_rating\"] = '' # If Anime ID exists in the movie dict...\n",
    "        except: pass # If the Anime ID doesn't exist in our record...\n",
    "#adding anime duration.\n",
    "for idx,_ in animelistDict.items():\n",
    "    try   : animelistDict[idx][\"duration\"] = AnimeList['duration'][AnimeList[anime_id]==idx]\n",
    "    except: \n",
    "        try   : animelistDict[idx][\"duration\"] = '' # If Anime ID exists in the movie dict...\n",
    "        except: pass # If the Anime ID doesn't exist in our record...\n",
    "#adding no of episodes.\n",
    "for idx,_ in animelistDict.items():\n",
    "    try   : animelistDict[idx][\"episodes\"] = AnimeList['episodes'][AnimeList[anime_id]==idx]\n",
    "    except: \n",
    "        try   : animelistDict[idx][\"episodes\"] = '' # If Anime ID exists in the movie dict...\n",
    "        except: pass # If the Anime ID doesn't exist in our record..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86b0ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Writing anime Data into the disk...\n",
      "[INFO] Writing Global Secondary Index Data into the disk...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(\"[INFO] Writing anime Data into the disk...\")\n",
    "with open('Data_Sets/dataFinal.json', 'w') as fp:\n",
    "    json.dump(animelistDict, fp, sort_keys=True, indent=4)\n",
    "print(\"[INFO] Writing Global Secondary Index Data into the disk...\")\n",
    "with open('Data_Sets/dataFinal_GIS.json', 'w') as fp:\n",
    "    json.dump(global_secondaryIndex, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f0b4a",
   "metadata": {},
   "source": [
    "### Now our database is ready and it can handel high inflow of requests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
